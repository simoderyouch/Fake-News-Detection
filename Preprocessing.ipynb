{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install farasapy\n",
    "!pip install tqdm\n",
    "!pip install gensim\n",
    "!pip install spacy\n",
    "!pip install scipy\n",
    "!pip install wandb\n",
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from time import time\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from farasa.stemmer import FarasaStemmer\n",
    "from farasa.segmenter import FarasaSegmenter\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import joblib\n",
    "from multiprocessing import Pool\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "arabic_stopwords = set(stopwords.words(\"arabic\"))\n",
    "farasa_stemmer = FarasaStemmer(interactive=True)\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, language='en'):\n",
    "        self.language = language.lower()\n",
    "        \n",
    "        if self.language == 'en':\n",
    "            self.stopwords = set(stopwords.words('english'))\n",
    "            self.stemmer = SnowballStemmer('english')\n",
    "        elif self.language == 'ar':\n",
    "            self.stopwords = self._load_arabic_stopwords()\n",
    "            self.farasa_segmenter = FarasaSegmenter(interactive=True)\n",
    "            self.farasa_stemmer = FarasaStemmer(interactive=True)\n",
    "        else:\n",
    "            raise ValueError(\"Language not supported. Choose 'english' or 'arabic'.\")\n",
    "\n",
    "        self.pattern_punctuation = re.compile(r'[^\\w\\s]')\n",
    "        self.pattern_digits = re.compile(r'\\d+')\n",
    "        self.pattern_spaces = re.compile(r'\\s+')\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "    def _load_arabic_stopwords(self):\n",
    "        return set(stopwords.words('arabic')) if 'arabic' in stopwords.fileids() else set()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean the text by removing punctuation, digits, and extra spaces.\"\"\"\n",
    "        text = self.pattern_punctuation.sub('', text)  \n",
    "        text = self.pattern_digits.sub('', text)  \n",
    "        text = self.pattern_spaces.sub(' ', text).strip()  \n",
    "        return text\n",
    "\n",
    "    def normalize_text(self, text):\n",
    "        \"\"\"Normalize text to lowercase.\"\"\"\n",
    "        text = text.lower()\n",
    "        if self.language == 'ar':\n",
    "            text = self._normalize_arabic(text)\n",
    "        return text\n",
    "\n",
    "    def _normalize_arabic(self, text):\n",
    "        \"\"\"Normalize Arabic text by replacing similar characters.\"\"\"\n",
    "        arabic_norm_map = {\n",
    "            'أ': 'ا', 'إ': 'ا', 'آ': 'ا',\n",
    "            'ة': 'ه',\n",
    "            'ي': 'ى',\n",
    "        }\n",
    "        return ''.join(arabic_norm_map.get(c, c) for c in text)\n",
    "\n",
    "    def remove_diacritics(self, text):\n",
    "        \"\"\"Remove Arabic diacritics (tashkeel).\"\"\"\n",
    "        return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "    def tokenize_text(self, text):\n",
    "        \"\"\"Tokenize text into words using SpaCy for English or Farasa for Arabic.\"\"\"\n",
    "        if self.language == 'en':\n",
    "            doc = nlp(text)\n",
    "            return [token.text for token in doc if token.is_alpha]  # Only alphabetic tokens\n",
    "        elif self.language == 'ar':\n",
    "            return self.farasa_segmenter.segment(text).split()\n",
    "\n",
    "    def remove_stopwords(self, tokens):\n",
    "        \"\"\"Remove stopwords from tokenized text.\"\"\"\n",
    "        return [word for word in tokens if word not in self.stopwords]\n",
    "\n",
    "    def stem_text(self, tokens):\n",
    "        \"\"\"Stem words using appropriate stemmer.\"\"\"\n",
    "        if self.language == 'en':\n",
    "            return [self.stemmer.stem(word) for word in tokens]\n",
    "        elif self.language == 'ar':\n",
    "            return [self.farasa_stemmer.stem(word) for word in tokens]\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        \"\"\"Apply all preprocessing steps.\"\"\"\n",
    "        text = self.clean_text(text)\n",
    "        text = self.normalize_text(text)\n",
    "        if self.language == 'ar':\n",
    "            text = self.remove_diacritics(text)\n",
    "        tokens = self.tokenize_text(text)\n",
    "        tokens = self.remove_stopwords(tokens)\n",
    "        tokens = self.stem_text(tokens)\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    \n",
    "    def parallel_preprocess(self, df, column_name, num_workers=4):\n",
    "        \"\"\"Apply preprocessing using ThreadPoolExecutor to a DataFrame column.\"\"\"\n",
    "        with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "            \n",
    "            result = list(tqdm(executor.map(self.preprocess_text, df[column_name]), \n",
    "                               total=len(df), \n",
    "                               desc=\"Processing Text Data\"))\n",
    "\n",
    "        \n",
    "        df[column_name] = result\n",
    "        return df\n",
    "\n",
    "    def preprocess(self, df, column_name):\n",
    "        \"\"\"Apply preprocessing to a Pandas DataFrame column using tqdm.\"\"\"\n",
    "        tqdm.pandas(desc=\"Processing Text Data\")\n",
    "        df[column_name] = df[column_name].progress_apply(self.preprocess_text)\n",
    "        return df\n",
    "\n",
    "\n",
    "    \n",
    "    def export_to_csv(self, df, filename):\n",
    "        \"\"\"Export the preprocessed DataFrame to a CSV file.\"\"\"\n",
    "        df.to_csv(filename, index=False, encoding='utf-8')\n",
    "        print(f\"Data exported successfully to {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en =  pd.read_csv('.\\Master_Data\\English Data\\WELFake_Dataset.csv')\n",
    "df_en.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_preprocessor = TextPreprocessor(language='en')\n",
    "df_en = eng_preprocessor.preprocess(df_en, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en['label'] = 1 - df_en['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en['label'] = df_en['label'].astype(int)\n",
    "\n",
    "df_en['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en['text_length'] = df_en['text'].str.len()\n",
    "print(df_en['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en = df_en[(df_en['text_length'] >= 50) & (df_en['text_length'] <= 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_en.dropna(inplace=True)\n",
    "df_en = df_en.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_preprocessor.export_to_csv(df_en , './Cleaned_Data/processed_en_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Arabic Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df_ar = pd.read_csv(\"hf://datasets/Nahla-yasmine/arabic_fake_news/final_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arb_preprocessor = TextPreprocessor(language='ar')\n",
    "df_ar = arb_preprocessor.preprocess(df_ar, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_map = {\n",
    "    'fake': 1,\n",
    "    'false': 1,\n",
    "    'real': 0,\n",
    "    'true': 0\n",
    "}\n",
    "\n",
    "df_ar['label'] = df_ar['label'].str.lower().map(label_map).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_distribution = df_ar['label'].value_counts()\n",
    "print(label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_real = df_ar[df_ar['label'] == 1]\n",
    "df_fake = df_ar[df_ar['label'] == 0]\n",
    "\n",
    "df_real_sampled = df_real.sample(len(df_fake), replace=True, random_state=42)\n",
    "\n",
    "df_balanced = pd.concat([df_fake, df_real_sampled])\n",
    "\n",
    "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(df_balanced['label'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar = df_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar['text_length'] = df_ar['text'].str.len()\n",
    "print(df_ar['text_length'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar = df_ar[(df_ar['text_length'] >= 20) & (df_ar['text_length'] <= 10000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ar.dropna(inplace=True)\n",
    "df_ar = df_ar.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arb_preprocessor.export_to_csv(df_en , './Cleaned_Data/processed_en_data.csv')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
